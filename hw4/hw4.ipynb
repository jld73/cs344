{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I think that deep neural networks are more of a breakthrough than a flash in the pan. While I'm sure that eventually they will be eclipsed by something more powerful, it seems like the result will be a more advanced version of them, rather than something else entirely. For example, one current avenue of research is in biocomputing, essentially hacking DNA to allow us to literally grow computers. This would allow us to grow actual neurons, rather than simulated ones. While the resulting artificial intelligence would probably not use the specific learning algorithms/architectures that we use today, it seems likely that they will be informed by our present technologies.\n",
    "The main area that I think could have the possibility for an entirely new breakthrough would be quantum cumputing. While I don't fully understand it myself, from what I've seen quantum computers require a completely different kind of programming that works well on certain problems, but is not able to effectively solve other problems. The new techniques required could lead to brand new discoveries that allow artificial intelligence to work much better than current models\n",
    "Overall, it's very hard to predict breakthroughs; if we could, would they really be \"breakthroughs\"? Whether or not deep networks are the best depends entirely on things that we do not know. However, given that deep learning in some sense models real intelligence in structure (through neurons), it seems like a powerful technique that will likely hold our attention for quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "((train_img, train_lbl), (test_img, test_lbl)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(train_lbl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_img = train_img.reshape((60000, 28*28))\n",
    "train_img = train_img.astype('float32') / 255\n",
    "\n",
    "test_img = test_img.reshape((10000, 28*28))\n",
    "test_img = test_img.astype('float32') /255\n",
    "\n",
    "train_lbl = to_categorical(train_lbl)\n",
    "test_lbl = to_categorical(test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation='tanh', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(64, activation='relu'))\n",
    "network.add(layers.Dense(64, activation='relu'))\n",
    "network.add(layers.Dense(32, activation='sigmoid'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.7326 - acc: 0.7777\n",
      "Epoch 2/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.4255 - acc: 0.8515\n",
      "Epoch 3/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.3761 - acc: 0.8661\n",
      "Epoch 4/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.3469 - acc: 0.8742\n",
      "Epoch 5/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.3263 - acc: 0.8808\n",
      "Epoch 6/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.3098 - acc: 0.8867\n",
      "Epoch 7/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2950 - acc: 0.8912\n",
      "Epoch 8/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2840 - acc: 0.8961\n",
      "Epoch 9/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2742 - acc: 0.8986\n",
      "Epoch 10/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2648 - acc: 0.9018\n",
      "Epoch 11/16\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.2556 - acc: 0.9055\n",
      "Epoch 12/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2481 - acc: 0.9076\n",
      "Epoch 13/16\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.2408 - acc: 0.9094\n",
      "Epoch 14/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2343 - acc: 0.9130\n",
      "Epoch 15/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2280 - acc: 0.9155\n",
      "Epoch 16/16\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2217 - acc: 0.9166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf6b887f60>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_img, train_lbl, epochs=16, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 74us/step\n",
      "test_acc: 0.8815\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_img, test_lbl)\n",
    "print('test_acc:', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best I was able to get was just above 88%. I tried changing the number of epohcs, the number and size of the layers, as well as the activation functions. I also tried to run a CNN, but unfortunately there seems to be some sort of issue with my environemnt as I have it set up right now. Every time I try to run it, it gives me an unknown error that says something about being unable to get the convolution algorithm, or an out of memory error. Based on some cursory research it appears that the lab machines may be running a bad version of tensorflow-gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
